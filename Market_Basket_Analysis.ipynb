{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "};"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv('products.csv')\n",
    "orders_df = pd.read_csv('orders.csv')\n",
    "#orders_products_prior_df = pd.read_csv('order_products__prior.csv')\n",
    "orders_products_train_df = pd.read_csv('order_products__train.csv')\n",
    "\n",
    "#orders_products_df = pd.concat([orders_products_prior_df, orders_products_train_df])\n",
    "orders_products_df = orders_products_train_df\n",
    "\n",
    "#orders_train_df = orders_df[orders_df['eval_set'].isin(['prior', 'train'])]\n",
    "orders_train_df = orders_df[orders_df['eval_set'].isin(['train'])]\n",
    "orders_test_df = orders_df[orders_df['eval_set'] == 'test']\n",
    "\n",
    "users_orders_products_train_df = reduce(lambda l,r: pd.merge(l,r,on='order_id'), [orders_products_df, orders_train_df])\n",
    "users_orders_products_test_df = reduce(lambda l,r: pd.merge(l,r,on='order_id'), [orders_products_df, orders_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = ['order_id','product_id','add_to_cart_order','user_id','order_number','order_dow', \n",
    "#                    'order_hour_of_day','days_since_prior_order']\n",
    "feature_columns = ['order_id','product_id','add_to_cart_order','user_id','order_number','order_dow', \n",
    "                   'order_hour_of_day','days_since_prior_order']\n",
    "X = users_orders_products_train_df[feature_columns]\n",
    "y = users_orders_products_train_df['reordered']\n",
    "\n",
    "imputer = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "#d = pd.get_dummies(users_orders_products_train_df, columns=['product_id'])\n",
    "\n",
    "#d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of reorder prediction using ANN(Layers=3): 59.82\n"
     ]
    }
   ],
   "source": [
    "my_ANN = MLPClassifier(hidden_layer_sizes=(100,10), activation= 'relu', \n",
    "                       solver='adam', alpha=1e-5, learning_rate_init = 0.01)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "my_ANN.fit(X_train, y_train)\n",
    "\n",
    "y_predict_ann = my_ANN.predict(X_test)\n",
    "\n",
    "score_ann = accuracy_score(y_test, y_predict_ann)\n",
    "\n",
    "print('Accuracy of reorder prediction using ANN(Layers=3):', format(score_ann*100, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10)]} \n",
      "\n",
      "0.5962085337743035\n",
      "{'hidden_layer_sizes': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# define a range for the \"number of neurons\" in the hidden layer for a network with 1 hidden layer:\n",
    "neuron_number = [(i,j) for i in range(1,11) for j in range(1,11)]                    \n",
    "\n",
    "# create a dictionary for grid parameter:\n",
    "param_grid = dict(hidden_layer_sizes = neuron_number)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "# instantiate the model:\n",
    "my_ANN = MLPClassifier(activation='logistic', solver='adam', alpha=1e-5, random_state=1, learning_rate_init = 0.1)\n",
    "\n",
    "# creat the grid, and define the metric for evaluating the model: \n",
    "grid = GridSearchCV(my_ANN, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the grid (start the grid search):\n",
    "grid.fit(X, y)\n",
    "\n",
    "# view the results:\n",
    "# print(grid.cv_results_)\n",
    "\n",
    "# view the best results corresponding to the best structure of ANN:\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.004\n",
    "min_confidence = 0.5\n",
    "\n",
    "# References: https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf\n",
    "#             https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    "def apriori(txn_set):\n",
    "    '''\n",
    "    Apriori algorithm implementation that returns the support and calculated association rules\n",
    "    Returns:\n",
    "        support    - dictionary of tuple of product ids and its calculated support value\n",
    "        rules_list - List of calculated rules where each rule is a dictionary with lhs of rule,\n",
    "                     rhs of rule and calculated confidence value for this rule.\n",
    "    '''\n",
    "    support_dict = {}\n",
    "    itemset_dict = get_all_single_item_count(txn_set)\n",
    "    txn_size = len(txn_set)\n",
    "    while len(itemset_dict) > 0:\n",
    "        support_dict = {**support_dict, **itemset_dict}\n",
    "        c_k = get_candidate_itemsets(itemset_dict)\n",
    "        if len(c_k) != 0:\n",
    "            size, ht = build_hash_tree(c_k)\n",
    "            for order_id in txn_set:\n",
    "                item_list = txn_set[order_id]\n",
    "                c_t = subset(size, ht, item_list)\n",
    "                for c in c_t:\n",
    "                    c_k[c] = c_k[c] + 1\n",
    "        #print('candidate_set',c_k)\n",
    "        itemset_dict = {k: v for k, v in c_k.items() if v/txn_size >= min_support}\n",
    "        #itemset_dict = {k: v for k, v in c_k.items() if v >= min_support}\n",
    "        #print('itemset_dict',c_k)\n",
    "        #support_dict = {**support_dict, **itemset_dict}\n",
    "        #print('support_dict',support_dict)\n",
    "    return support_dict, apriori_genrules(support_dict)\n",
    "\n",
    "def apriori_genrules(support_dict):\n",
    "    '''\n",
    "    Function that generates rules based on the support. If a rule like (a,b,c)->(d) has\n",
    "    low confidence then the function skips processing all other rules (a,b)->(c,d)\n",
    "    and (a)->(b,c,d) for efficiency.\n",
    "    Confidence of each rule of form (a)->(b) is support(a)/support(b)\n",
    "    '''\n",
    "    rules_list = []\n",
    "    for itemset_tuple in support_dict:\n",
    "        itemset_list = list(itemset_tuple)\n",
    "        itemset_size = len(itemset_list)\n",
    "        if itemset_size >= 2:\n",
    "            for i in range(itemset_size-1, 1, -1):\n",
    "                lhs = tuple(itemset_list[0:i])\n",
    "                rhs = tuple(itemset_list[i:itemset_size])\n",
    "                conf = support_dict[itemset_tuple]/support_dict[lhs]\n",
    "                if conf >= min_confidence:\n",
    "                    rule_dict = {}\n",
    "                    rule_dict['lhs'] = lhs\n",
    "                    rule_dict['rhs'] = rhs\n",
    "                    rule_dict['conf'] = conf\n",
    "                    rules_list.append(rule_dict)\n",
    "                else:\n",
    "                    break\n",
    "    return rules_list\n",
    "\n",
    "def get_all_single_item_count(txn_set):\n",
    "    '''\n",
    "    Returns all unique product ids from transactions and the count of occurrence of\n",
    "    each product where count of occurrence(support) is above the given min_support.\n",
    "    '''\n",
    "    itemset_count_dict = {}\n",
    "    txn_size = len(txn_set)\n",
    "    for order_id in txn_set:\n",
    "        itemset = txn_set[order_id]\n",
    "        for item in itemset:\n",
    "            if item not in itemset_count_dict:\n",
    "                itemset_count_dict[item] = 0\n",
    "            itemset_count_dict[item] = itemset_count_dict[item] + 1\n",
    "    return {(k,): v for k, v in itemset_count_dict.items() if v/txn_size >= min_support}\n",
    "    #return {(k,): v for k, v in itemset_count_dict.items() if v >= min_support}\n",
    "\n",
    "def get_candidate_itemsets(itemset_dict):\n",
    "    '''\n",
    "    Combines a given list of item set with itself and generated a new list as the candidate list\n",
    "    Only items sets whose last item differ and all other last but one product ids are same are combined. \n",
    "    '''\n",
    "    c_set = {}\n",
    "    itemset_list = [k for k,v in itemset_dict.items()]\n",
    "    itemset_size = len(itemset_list)\n",
    "    for i in range(0, itemset_size-1):\n",
    "        l1 = itemset_list[i]\n",
    "        l1_len = len(l1)\n",
    "        for j in range(i+1, itemset_size):\n",
    "            l2 = itemset_list[j]\n",
    "            l2_len = len(l2)\n",
    "            if l1[0:l1_len-1] == l2[0:l2_len-1] and l1[-1] != l2[-1]:\n",
    "                c = l1 + l2[-1:]\n",
    "                c_set[tuple(sorted(c))] = 0\n",
    "    return c_set\n",
    "                \n",
    "class Node:\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.data = []\n",
    "        self.children = {}\n",
    "        \n",
    "    def addData(self, d):\n",
    "        self.data.append(d)\n",
    "    \n",
    "    def addChild(self, childKey):\n",
    "        if childKey not in self.children:\n",
    "            self.children[childKey] = Node(childKey)\n",
    "        return self.children[childKey]\n",
    "    \n",
    "    def getChild(self, childKey):\n",
    "        if childKey in self.children:\n",
    "            return self.children[childKey]\n",
    "        return None\n",
    "    \n",
    "    def isLeafNode(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def nodeToString(self, indent):\n",
    "        s = indent+'key:'+(str(self.key) if self.key is not None else 'None')+'\\n'\n",
    "        data = '),('.join(','.join(str(t) for t in d) for d in self.data)\n",
    "        s = s+indent+'data:[('+data+')]\\n'\n",
    "        s = s+indent+'isLeafNode:'+str(self.isLeafNode())+'\\n'\n",
    "        l = len(self.children)\n",
    "        if l==0:\n",
    "            s=s+indent+'children:{}\\n'\n",
    "        else:\n",
    "            s=s+indent+'children:{\\n'\n",
    "            for (key,val) in self.children.items():\n",
    "                s=s+val.nodeToString(indent+'  ')\n",
    "            s=s+indent+'}\\n'\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "         return self.nodeToString('')\n",
    "        \n",
    "def build_hash_tree(c_k):\n",
    "    '''\n",
    "    Builds a hash tree with given candidate item set. The max number of branches at each level will be\n",
    "    the size of the item set. mod(product_id, item_set_size) will assign the product id to its\n",
    "    respective branch.\n",
    "    '''\n",
    "    hash_tree = {}\n",
    "    size = 0\n",
    "    for itemset in c_k:\n",
    "        size = len(itemset)\n",
    "        keyset = []\n",
    "        for item in itemset:\n",
    "            keyset.append(item % size)\n",
    "        keyset = tuple(keyset)\n",
    "        if keyset not in hash_tree:\n",
    "            hash_tree[keyset] = []\n",
    "        hash_tree[keyset].append(itemset)\n",
    "    root = Node(None)\n",
    "    for keyset in hash_tree:\n",
    "        n = root\n",
    "        for key in keyset:\n",
    "            n = n.addChild(key)\n",
    "        for itemset in hash_tree[keyset]:\n",
    "            n.addData(itemset)        \n",
    "    return size, root\n",
    "      \n",
    "def subset(size, ht, item_list):\n",
    "    '''\n",
    "    Returns all subset candidate item sets of a given item_list.\n",
    "    This function that collects all possible subsets of given size in the bigger set item_list\n",
    "    This function optimizes the subset generation with the knowledge of the minimum size of subset\n",
    "    that should be generated. For example no of 3 item subsets of [1,2,3,4,5] are:\n",
    "        123\n",
    "        124\n",
    "        125\n",
    "        134\n",
    "        135\n",
    "        145\n",
    "        234\n",
    "        235\n",
    "        245\n",
    "        345\n",
    "    Out of all possible subsets, the function only returns those that are part of the hash tree\n",
    "    '''\n",
    "    subset_tuple_list = []\n",
    "    item_list = sorted(item_list)\n",
    "    item_size = len(item_list)\n",
    "    if(item_size >= size):\n",
    "        comb_tuples = combinations(item_list, size)\n",
    "        for comb_tuple in comb_tuples:\n",
    "            node = ht\n",
    "            for comb_item in comb_tuple:\n",
    "                node = node.getChild(comb_item%size)\n",
    "                if node is None:\n",
    "                    break\n",
    "            if node is not None and node.isLeafNode():\n",
    "                if comb_tuple in node.data:\n",
    "                    subset_tuple_list.append(comb_tuple)\n",
    "    return subset_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_txn_set():\n",
    "    txn_set = {}\n",
    "    for index, row in users_orders_products_train_df.iterrows():\n",
    "        order_id = row['order_id']\n",
    "        product_id = row['product_id']\n",
    "        if order_id not in txn_set:\n",
    "            txn_set[order_id] = []\n",
    "        if product_id not in txn_set[order_id]:\n",
    "            txn_set[order_id].append(product_id)\n",
    "    return txn_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_set = prepare_txn_set()\n",
    "support, rules = apriori(txn_set)\n",
    "\n",
    "print(support)\n",
    "print(rules)\n",
    "\n",
    "for r in rules:\n",
    "    lhs = r['lhs']\n",
    "    rhs = r['rhs']\n",
    "    conf = r['conf']\n",
    "    s = '('\n",
    "    for p in lhs:\n",
    "        s = s + products_df[products_df['product_id'] == p].iloc[0]['product_name'] + ','\n",
    "    s = s[:-1] + ') -> ('\n",
    "    for p in rhs:\n",
    "        s = s + products_df[products_df['product_id'] == p].iloc[0]['product_name'] + ','\n",
    "    s = s[:-1] + ') : ' + str(format(conf, '.2f'))\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "};"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv('products.csv')\n",
    "orders_df = pd.read_csv('orders.csv')\n",
    "#orders_products_prior_df = pd.read_csv('order_products__prior.csv')\n",
    "orders_products_train_df = pd.read_csv('order_products__train.csv')\n",
    "\n",
    "#orders_products_df = pd.concat([orders_products_prior_df, orders_products_train_df])\n",
    "orders_products_df = orders_products_train_df\n",
    "\n",
    "#orders_train_df = orders_df[orders_df['eval_set'].isin(['prior', 'train'])]\n",
    "orders_train_df = orders_df[orders_df['eval_set'].isin(['train'])]\n",
    "orders_test_df = orders_df[orders_df['eval_set'] == 'test']\n",
    "\n",
    "users_orders_products_train_df = reduce(lambda l,r: pd.merge(l,r,on='order_id'), [orders_products_df, orders_train_df])\n",
    "users_orders_products_test_df = reduce(lambda l,r: pd.merge(l,r,on='order_id'), [orders_products_df, orders_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = ['order_id','product_id','add_to_cart_order','user_id','order_number','order_dow', \n",
    "#                    'order_hour_of_day','days_since_prior_order']\n",
    "feature_columns = ['order_id','product_id','add_to_cart_order','user_id','order_number','order_dow', \n",
    "                   'order_hour_of_day','days_since_prior_order']\n",
    "X = users_orders_products_train_df[feature_columns]\n",
    "y = users_orders_products_train_df['reordered']\n",
    "\n",
    "imputer = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "#d = pd.get_dummies(users_orders_products_train_df, columns=['product_id'])\n",
    "\n",
    "#d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of reorder prediction using ANN(Layers=3): 59.82\n"
     ]
    }
   ],
   "source": [
    "my_ANN = MLPClassifier(hidden_layer_sizes=(100,10), activation= 'relu', \n",
    "                       solver='adam', alpha=1e-5, learning_rate_init = 0.01)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "my_ANN.fit(X_train, y_train)\n",
    "\n",
    "y_predict_ann = my_ANN.predict(X_test)\n",
    "\n",
    "score_ann = accuracy_score(y_test, y_predict_ann)\n",
    "\n",
    "print('Accuracy of reorder prediction using ANN(Layers=3):', format(score_ann*100, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10)]} \n",
      "\n",
      "0.5962085337743035\n",
      "{'hidden_layer_sizes': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# define a range for the \"number of neurons\" in the hidden layer for a network with 1 hidden layer:\n",
    "neuron_number = [(i,j) for i in range(1,11) for j in range(1,11)]                    \n",
    "\n",
    "# create a dictionary for grid parameter:\n",
    "param_grid = dict(hidden_layer_sizes = neuron_number)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "# instantiate the model:\n",
    "my_ANN = MLPClassifier(activation='logistic', solver='adam', alpha=1e-5, random_state=1, learning_rate_init = 0.1)\n",
    "\n",
    "# creat the grid, and define the metric for evaluating the model: \n",
    "grid = GridSearchCV(my_ANN, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the grid (start the grid search):\n",
    "grid.fit(X, y)\n",
    "\n",
    "# view the results:\n",
    "# print(grid.cv_results_)\n",
    "\n",
    "# view the best results corresponding to the best structure of ANN:\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.004\n",
    "min_confidence = 0.5\n",
    "\n",
    "# References: https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf\n",
    "#             https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    "def apriori(txn_set):\n",
    "    '''\n",
    "    Apriori algorithm implementation that returns the support and calculated association rules\n",
    "    Returns:\n",
    "        support    - dictionary of tuple of product ids and its calculated support value\n",
    "        rules_list - List of calculated rules where each rule is a dictionary with lhs of rule,\n",
    "                     rhs of rule and calculated confidence value for this rule.\n",
    "    '''\n",
    "    support_dict = {}\n",
    "    itemset_dict = get_all_single_item_count(txn_set)\n",
    "    txn_size = len(txn_set)\n",
    "    while len(itemset_dict) > 0:\n",
    "        support_dict = {**support_dict, **itemset_dict}\n",
    "        c_k = get_candidate_itemsets(itemset_dict)\n",
    "        if len(c_k) != 0:\n",
    "            size, ht = build_hash_tree(c_k)\n",
    "            for order_id in txn_set:\n",
    "                item_list = txn_set[order_id]\n",
    "                c_t = subset(size, ht, item_list)\n",
    "                for c in c_t:\n",
    "                    c_k[c] = c_k[c] + 1\n",
    "        #print('candidate_set',c_k)\n",
    "        itemset_dict = {k: v for k, v in c_k.items() if v/txn_size >= min_support}\n",
    "        #itemset_dict = {k: v for k, v in c_k.items() if v >= min_support}\n",
    "        #print('itemset_dict',c_k)\n",
    "        #support_dict = {**support_dict, **itemset_dict}\n",
    "        #print('support_dict',support_dict)\n",
    "    return support_dict, apriori_genrules(support_dict)\n",
    "\n",
    "def apriori_genrules(support_dict):\n",
    "    '''\n",
    "    Function that generates rules based on the support. If a rule like (a,b,c)->(d) has\n",
    "    low confidence then the function skips processing all other rules (a,b)->(c,d)\n",
    "    and (a)->(b,c,d) for efficiency.\n",
    "    Confidence of each rule of form (a)->(b) is support(a)/support(b)\n",
    "    '''\n",
    "    rules_list = []\n",
    "    for itemset_tuple in support_dict:\n",
    "        itemset_list = list(itemset_tuple)\n",
    "        itemset_size = len(itemset_list)\n",
    "        if itemset_size >= 2:\n",
    "            for i in range(itemset_size-1, 1, -1):\n",
    "                lhs = tuple(itemset_list[0:i])\n",
    "                rhs = tuple(itemset_list[i:itemset_size])\n",
    "                conf = support_dict[itemset_tuple]/support_dict[lhs]\n",
    "                if conf >= min_confidence:\n",
    "                    rule_dict = {}\n",
    "                    rule_dict['lhs'] = lhs\n",
    "                    rule_dict['rhs'] = rhs\n",
    "                    rule_dict['conf'] = conf\n",
    "                    rules_list.append(rule_dict)\n",
    "                else:\n",
    "                    break\n",
    "    return rules_list\n",
    "\n",
    "def get_all_single_item_count(txn_set):\n",
    "    '''\n",
    "    Returns all unique product ids from transactions and the count of occurrence of\n",
    "    each product where count of occurrence(support) is above the given min_support.\n",
    "    '''\n",
    "    itemset_count_dict = {}\n",
    "    txn_size = len(txn_set)\n",
    "    for order_id in txn_set:\n",
    "        itemset = txn_set[order_id]\n",
    "        for item in itemset:\n",
    "            if item not in itemset_count_dict:\n",
    "                itemset_count_dict[item] = 0\n",
    "            itemset_count_dict[item] = itemset_count_dict[item] + 1\n",
    "    return {(k,): v for k, v in itemset_count_dict.items() if v/txn_size >= min_support}\n",
    "    #return {(k,): v for k, v in itemset_count_dict.items() if v >= min_support}\n",
    "\n",
    "def get_candidate_itemsets(itemset_dict):\n",
    "    '''\n",
    "    Combines a given list of item set with itself and generated a new list as the candidate list\n",
    "    Only items sets whose last item differ and all other last but one product ids are same are combined. \n",
    "    '''\n",
    "    c_set = {}\n",
    "    itemset_list = [k for k,v in itemset_dict.items()]\n",
    "    itemset_size = len(itemset_list)\n",
    "    for i in range(0, itemset_size-1):\n",
    "        l1 = itemset_list[i]\n",
    "        l1_len = len(l1)\n",
    "        for j in range(i+1, itemset_size):\n",
    "            l2 = itemset_list[j]\n",
    "            l2_len = len(l2)\n",
    "            if l1[0:l1_len-1] == l2[0:l2_len-1] and l1[-1] != l2[-1]:\n",
    "                c = l1 + l2[-1:]\n",
    "                c_set[tuple(sorted(c))] = 0\n",
    "    return c_set\n",
    "                \n",
    "class Node:\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.data = []\n",
    "        self.children = {}\n",
    "        \n",
    "    def addData(self, d):\n",
    "        self.data.append(d)\n",
    "    \n",
    "    def addChild(self, childKey):\n",
    "        if childKey not in self.children:\n",
    "            self.children[childKey] = Node(childKey)\n",
    "        return self.children[childKey]\n",
    "    \n",
    "    def getChild(self, childKey):\n",
    "        if childKey in self.children:\n",
    "            return self.children[childKey]\n",
    "        return None\n",
    "    \n",
    "    def isLeafNode(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def nodeToString(self, indent):\n",
    "        s = indent+'key:'+(str(self.key) if self.key is not None else 'None')+'\\n'\n",
    "        data = '),('.join(','.join(str(t) for t in d) for d in self.data)\n",
    "        s = s+indent+'data:[('+data+')]\\n'\n",
    "        s = s+indent+'isLeafNode:'+str(self.isLeafNode())+'\\n'\n",
    "        l = len(self.children)\n",
    "        if l==0:\n",
    "            s=s+indent+'children:{}\\n'\n",
    "        else:\n",
    "            s=s+indent+'children:{\\n'\n",
    "            for (key,val) in self.children.items():\n",
    "                s=s+val.nodeToString(indent+'  ')\n",
    "            s=s+indent+'}\\n'\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "         return self.nodeToString('')\n",
    "        \n",
    "def build_hash_tree(c_k):\n",
    "    '''\n",
    "    Builds a hash tree with given candidate item set. The max number of branches at each level will be\n",
    "    the size of the item set. mod(product_id, item_set_size) will assign the product id to its\n",
    "    respective branch.\n",
    "    '''\n",
    "    hash_tree = {}\n",
    "    size = 0\n",
    "    for itemset in c_k:\n",
    "        size = len(itemset)\n",
    "        keyset = []\n",
    "        for item in itemset:\n",
    "            keyset.append(item % size)\n",
    "        keyset = tuple(keyset)\n",
    "        if keyset not in hash_tree:\n",
    "            hash_tree[keyset] = []\n",
    "        hash_tree[keyset].append(itemset)\n",
    "    root = Node(None)\n",
    "    for keyset in hash_tree:\n",
    "        n = root\n",
    "        for key in keyset:\n",
    "            n = n.addChild(key)\n",
    "        for itemset in hash_tree[keyset]:\n",
    "            n.addData(itemset)        \n",
    "    return size, root\n",
    "      \n",
    "def subset(size, ht, item_list):\n",
    "    '''\n",
    "    Returns all subset candidate item sets of a given item_list.\n",
    "    This function that collects all possible subsets of given size in the bigger set item_list\n",
    "    This function optimizes the subset generation with the knowledge of the minimum size of subset\n",
    "    that should be generated. For example no of 3 item subsets of [1,2,3,4,5] are:\n",
    "        123\n",
    "        124\n",
    "        125\n",
    "        134\n",
    "        135\n",
    "        145\n",
    "        234\n",
    "        235\n",
    "        245\n",
    "        345\n",
    "    Out of all possible subsets, the function only returns those that are part of the hash tree\n",
    "    '''\n",
    "    subset_tuple_list = []\n",
    "    item_list = sorted(item_list)\n",
    "    item_size = len(item_list)\n",
    "    if(item_size >= size):\n",
    "        comb_tuples = combinations(item_list, size)\n",
    "        for comb_tuple in comb_tuples:\n",
    "            node = ht\n",
    "            for comb_item in comb_tuple:\n",
    "                node = node.getChild(comb_item%size)\n",
    "                if node is None:\n",
    "                    break\n",
    "            if node is not None and node.isLeafNode():\n",
    "                if comb_tuple in node.data:\n",
    "                    subset_tuple_list.append(comb_tuple)\n",
    "    return subset_tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_txn_set():\n",
    "    txn_set = {}\n",
    "    for index, row in users_orders_products_train_df.iterrows():\n",
    "        order_id = row['order_id']\n",
    "        product_id = row['product_id']\n",
    "        if order_id not in txn_set:\n",
    "            txn_set[order_id] = []\n",
    "        if product_id not in txn_set[order_id]:\n",
    "            txn_set[order_id].append(product_id)\n",
    "    return txn_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(21616,): 700, (28842,): 445, (42625,): 169, (30391,): 1077, (40706,): 896, (27966,): 1279, (24489,): 268, (39275,): 1126, (8859,): 221, (13176,): 3522, (35951,): 520, (35042,): 167, (30776,): 196, (36695,): 227, (27344,): 428, (24964,): 980, (34126,): 552, (22935,): 1019, (45007,): 1062, (25659,): 246, (28199,): 301, (24852,): 4357, (1940,): 230, (21137,): 2520, (41220,): 276, (14992,): 252, (21405,): 246, (11182,): 281, (28289,): 180, (9387,): 328, (45066,): 822, (21903,): 2286, (47209,): 1705, (9076,): 566, (27104,): 625, (48679,): 598, (10749,): 573, (31717,): 841, (30720,): 159, (16759,): 412, (5876,): 815, (8193,): 332, (43789,): 276, (26209,): 1383, (40174,): 152, (31343,): 232, (26369,): 293, (43961,): 577, (33000,): 367, (28985,): 615, (34969,): 484, (6046,): 232, (5450,): 755, (31506,): 506, (33129,): 155, (31553,): 286, (29487,): 420, (47734,): 171, (21938,): 584, (48745,): 176, (432,): 343, (47626,): 1841, (8424,): 697, (44632,): 795, (8277,): 503, (38739,): 364, (34358,): 439, (5077,): 525, (20995,): 300, (44910,): 180, (42265,): 849, (37687,): 357, (18465,): 436, (22035,): 454, (33731,): 406, (32734,): 154, (31040,): 178, (40604,): 347, (4920,): 897, (24184,): 666, (24799,): 165, (6187,): 154, (4562,): 174, (43772,): 242, (35221,): 474, (37067,): 525, (27845,): 1121, (13535,): 171, (38383,): 201, (42768,): 283, (27521,): 426, (3957,): 294, (47672,): 185, (21019,): 197, (28934,): 155, (41787,): 266, (25890,): 501, (27156,): 331, (17794,): 595, (35108,): 182, (42585,): 256, (16185,): 286, (41844,): 276, (22963,): 279, (35921,): 184, (34243,): 321, (9839,): 417, (46802,): 259, (38293,): 237, (43352,): 743, (4799,): 288, (28204,): 734, (16797,): 1479, (11777,): 339, (8518,): 866, (46667,): 434, (42736,): 366, (20114,): 431, (10673,): 153, (11520,): 431, (2295,): 301, (3952,): 175, (36011,): 188, (37646,): 625, (19057,): 691, (46906,): 333, (23375,): 173, (32655,): 229, (4210,): 313, (25466,): 168, (42450,): 158, (21288,): 249, (46979,): 889, (24830,): 243, (23288,): 201, (33198,): 409, (47766,): 1661, (32177,): 156, (48364,): 214, (30233,): 257, (2966,): 157, (40310,): 203, (196,): 364, (49683,): 557, (6184,): 333, (23909,): 364, (44987,): 181, (48205,): 218, (19678,): 276, (39877,): 529, (44359,): 605, (7781,): 308, (43086,): 153, (19048,): 154, (17461,): 153, (26604,): 644, (31683,): 200, (30489,): 661, (4605,): 860, (46654,): 199, (12341,): 513, (21709,): 338, (43768,): 237, (39619,): 174, (32864,): 220, (26940,): 169, (41665,): 156, (14947,): 258, (1463,): 207, (19660,): 495, (41950,): 577, (5025,): 336, (19348,): 300, (25146,): 224, (18362,): 221, (15937,): 183, (28849,): 307, (33120,): 199, (12206,): 163, (7948,): 194, (15290,): 447, (40516,): 152, (42701,): 252, (23165,): 175, (21267,): 156, (13870,): 240, (13984,): 156, (39928,): 416, (33754,): 230, (44422,): 160, (32478,): 176, (49235,): 593, (8571,): 157, (30450,): 207, (20119,): 177, (10246,): 240, (17948,): 252, (18531,): 193, (39475,): 210, (5785,): 379, (8021,): 297, (43713,): 178, (36070,): 178, (46969,): 169, (5212,): 194, (43122,): 229, (41290,): 177, (17600,): 195, (2078,): 192, (34448,): 156, (29987,): 231, (20842,): 260, (40723,): 167, (23734,): 235, (42342,): 217, (10132,): 158, (24838,): 503, (39408,): 226, (890,): 179, (2086,): 188, (38689,): 274, (260,): 256, (26790,): 162, (42828,): 260, (35547,): 155, (21376,): 182, (38400,): 157, (7628,): 155, (43154,): 157, (1158,): 185, (8174,): 448, (18523,): 192, (27086,): 546, (39812,): 172, (39993,): 160, (22825,): 434, (38456,): 182, (47144,): 330, (5258,): 153, (5479,): 255, (26620,): 225, (43295,): 300, (44142,): 362, (17122,): 160, (38159,): 169, (26165,): 203, (44683,): 255, (5818,): 204, (32689,): 244, (35939,): 205, (40545,): 162, (40332,): 156, (14678,): 183, (47630,): 185, (13629,): 198, (7021,): 171, (30827,): 160, (16953,): 209, (41149,): 192, (46676,): 226, (16349,): 166, (13176, 30391): 299, (24852, 30391): 179, (21137, 30391): 243, (21903, 30391): 216, (30391, 47209): 199, (30391, 47626): 160, (13176, 40706): 200, (24852, 40706): 200, (21137, 40706): 165, (21903, 40706): 185, (40706, 47766): 174, (27966, 39275): 157, (13176, 27966): 406, (24852, 27966): 201, (21137, 27966): 398, (21903, 27966): 171, (27966, 47209): 232, (13176, 39275): 263, (24852, 39275): 194, (21137, 39275): 283, (21903, 39275): 163, (13176, 24964): 201, (13176, 22935): 235, (13176, 45007): 241, (13176, 21137): 708, (13176, 21903): 502, (13176, 47209): 534, (13176, 31717): 156, (5876, 13176): 216, (13176, 26209): 191, (13176, 47626): 224, (13176, 42265): 175, (4920, 13176): 154, (13176, 27845): 267, (13176, 16797): 156, (8518, 13176): 172, (13176, 37646): 174, (13176, 19057): 228, (13176, 46979): 166, (13176, 47766): 236, (13176, 39877): 152, (13176, 39928): 160, (22935, 24964): 185, (24852, 24964): 156, (21137, 24964): 158, (21903, 24964): 169, (24964, 47209): 170, (24964, 26209): 165, (24964, 47626): 175, (21137, 22935): 159, (21903, 22935): 161, (22935, 47209): 168, (24852, 45007): 216, (21137, 45007): 175, (21903, 45007): 210, (45007, 47626): 174, (21137, 24852): 498, (24852, 45066): 298, (21903, 24852): 455, (24852, 47209): 235, (9076, 24852): 186, (24852, 26209): 299, (24852, 28985): 156, (5450, 24852): 215, (24852, 47626): 477, (8424, 24852): 204, (24852, 44632): 171, (24852, 42265): 179, (4920, 24852): 262, (24184, 24852): 191, (24852, 27845): 236, (24852, 28204): 282, (16797, 24852): 447, (8518, 24852): 155, (24852, 46979): 196, (24852, 47766): 504, (24852, 49683): 183, (24852, 26604): 159, (24852, 30489): 159, (4605, 24852): 236, (21137, 21903): 365, (21137, 47209): 361, (5876, 21137): 159, (21137, 26209): 178, (21137, 47626): 227, (21137, 42265): 171, (4920, 21137): 155, (21137, 27845): 229, (19057, 21137): 169, (21137, 47766): 234, (21903, 47209): 300, (21903, 31717): 161, (21903, 26209): 224, (21903, 47626): 264, (21903, 42265): 153, (21903, 27845): 155, (8518, 21903): 168, (21903, 46979): 156, (21903, 47766): 324, (5876, 47209): 191, (26209, 47209): 186, (47209, 47626): 176, (26209, 31717): 234, (26209, 47626): 368, (26209, 47766): 234, (16797, 47626): 154, (8518, 47626): 165, (46979, 47626): 153, (47626, 47766): 288, (4605, 47626): 174, (16797, 43352): 159, (13176, 21137, 27966): 159, (13176, 21137, 47209): 171}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "txn_set = prepare_txn_set()\n",
    "support, rules = apriori(txn_set)\n",
    "\n",
    "print(support)\n",
    "print(rules)\n",
    "\n",
    "for r in rules:\n",
    "    lhs = r['lhs']\n",
    "    rhs = r['rhs']\n",
    "    conf = r['conf']\n",
    "    s = '('\n",
    "    for p in lhs:\n",
    "        s = s + products_df[products_df['product_id'] == p].iloc[0]['product_name'] + ','\n",
    "    s = s[:-1] + ') -> ('\n",
    "    for p in rhs:\n",
    "        s = s + products_df[products_df['product_id'] == p].iloc[0]['product_name'] + ','\n",
    "    s = s[:-1] + ') : ' + str(format(conf, '.2f'))\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
